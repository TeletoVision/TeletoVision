{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 256\n",
    "CHUNK_OVERLAP = 128\n",
    "\n",
    "EMBEDDING_MODEL = \"intfloat/e5-large\"\n",
    "\n",
    "K = 2\n",
    "\n",
    "LLM = \"google/gemma-2-9b-it\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an AI visual assistant surveillance operator that can analyze real-time traffic analysis and accident detection.\n",
    "\n",
    "Respond to user's questions as accurately as possible.\n",
    "Be careful not to answer with false information.\n",
    "\n",
    "Using the provided caption information, describe the scene in a detailed manner.\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "QUANTIZATION = \"bf16\" # \"qlora\", \"bf16\", \"fp16\"\n",
    "\n",
    "MAX_NEW_TOKENS = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json(file_path, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP):\n",
    "    loader = JSONLoader(\n",
    "        file_path=file_path,\n",
    "        jq_schema=\".frame.[].caption\",\n",
    "        text_content=False,\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    chunks = docs.copy()\n",
    "    return chunks\n",
    "\n",
    "def create_vector_db(chunks, model_path=EMBEDDING_MODEL):\n",
    "    \"\"\"FAISS DB\"\"\"\n",
    "    model_kwargs = {'device': 'cpu'}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_path,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    db = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    return db\n",
    "    \n",
    "def process_jsons_from_dataframe(unique_paths, base_directory):\n",
    "    \n",
    "    json_databases = {}\n",
    "    \n",
    "    for path in tqdm(unique_paths, desc=\"Processing JSONs\"):\n",
    "        \n",
    "        normalized_path = unicodedata.normalize('NFC', path)\n",
    "        full_path = os.path.normpath(\n",
    "            os.path.join(\n",
    "                base_directory, normalized_path.lstrip('./')\n",
    "            )\n",
    "        ) if not os.path.isabs(normalized_path) else normalized_path\n",
    "        json_title = os.path.splitext(os.path.basename(full_path))[0]\n",
    "        \n",
    "        print(f\"Processing {json_title}...\")\n",
    "        \n",
    "        chunks = process_json(full_path)\n",
    "        db = create_vector_db(chunks)\n",
    "        \n",
    "        # Retriever\n",
    "        \n",
    "        retriever_similarity = db.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={'k': K}\n",
    "        )\n",
    "\n",
    "        retriever_mmr = db.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={'k': K}\n",
    "        )\n",
    "\n",
    "        retriever_bm25 = BM25Retriever.from_documents(chunks)\n",
    "        \n",
    "        retriever = EnsembleRetriever(\n",
    "            retrievers=[retriever_similarity, retriever_mmr, retriever_bm25],\n",
    "            weights=[0.5, 0.5, 0.5]\n",
    "        )        \n",
    "        \n",
    "        json_databases[json_title] = {\n",
    "                'db': db,\n",
    "                'retriever': retriever\n",
    "        }\n",
    "    return json_databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cam_04.mp4-verb.json',\n",
       " 'cam_05.mp4-verb.json',\n",
       " 'cam_06.mp4-verb.json',\n",
       " 'cam_07.mp4-verb.json']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_directory = 'data/retrieval/' # Your Base Directory\n",
    "os.listdir(base_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSONs:   0%|                                                                       | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cam_04.mp4-verb...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSONs: 100%|███████████████████████████████████████████████████████████████| 1/1 [00:24<00:00, 24.90s/it]\n"
     ]
    }
   ],
   "source": [
    "base_directory = 'data/retrieval/' # Your Base Directory\n",
    "unique_paths = os.listdir(base_directory)[:1]\n",
    "json_databases = process_jsons_from_dataframe(unique_paths, base_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_llm_pipeline():\n",
    "    model_id = LLM\n",
    "    quantization_options = {\n",
    "        \"qlora\": {\"quantization_config\": BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,\n",
    "                    bnb_4bit_use_double_quant=True,\n",
    "                    bnb_4bit_quant_type=\"nf4\",\n",
    "                    bnb_4bit_compute_dtype=torch.bfloat16)},\n",
    "        \"bf16\": {\"torch_dtype\": torch.bfloat16},\n",
    "        \"fp16\": {\"torch_dtype\": \"float16\"}\n",
    "    }\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        **quantization_options.get(QUANTIZATION, {})\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.use_default_system_prompt = False\n",
    "\n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        # temperature=0.2,\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        # do_sample=True,\n",
    "    )\n",
    "\n",
    "    hf = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "    return hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55806353c3b41b197ac1e93d3505e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "llm = setup_llm_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_sample = 'person lying on the ground'\n",
    "source = 'cam_04.mp4-verb'\n",
    "\n",
    "docs = json_databases[source]['retriever'].invoke(prompt_sample)\n",
    "\n",
    "for doc in docs:\n",
    "    print(f\"frame : {doc.metadata['seq_num']-1}\")\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain 을 이용한 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Find the frame with a person lying on the floor\n",
      "Answer: The person lying on the floor is visible in frames 8, 13, and 11. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = json_databases[source]['retriever'].invoke(prompt_sample)\n",
    "\n",
    "def format_docs(docs):\n",
    "    context = \"\"\n",
    "    for doc in docs:\n",
    "        context += f\"frame : {doc.metadata['seq_num']-1}\"\n",
    "        context += '\\n'\n",
    "        context += doc.page_content\n",
    "        context += '\\n'\n",
    "    return context\n",
    "\n",
    "results = []\n",
    "\n",
    "question = 'Find the frame with a person lying on the floor'\n",
    "source = 'cam_04.mp4-verb'\n",
    "\n",
    "retriever = json_databases[source]['retriever']\n",
    "\n",
    "template = PROMPT_TEMPLATE\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# RAG 체인 정의\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "full_response = rag_chain.invoke(question)\n",
    "\n",
    "print(f\"Answer: {full_response}\\n\")\n",
    "\n",
    "results.append({\n",
    "    \"Question\": question,\n",
    "    \"Answer\": full_response\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
